{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Continual Anomaly Detection Experiments\n",
    "\n",
    "---\n",
    "\n",
    "### Datasets\n",
    "\n",
    "We have 2 datasets, MVTec-AD for benchmarking the traditional continual learning methods under the usual framework, where each category/object is its own task, and MTD, which contains one object type, Magnetic Tile Defects.\n",
    "\n",
    "The magnetic tiles have 5 different kinds of defects, and we will use this dataset to test out how benchmark methods perform in a continous data drift scenario. We will run several experiments on the MTD dataset, where each experiment will have _T_ tasks. The tasks will be progressive disruptions to the image at increasing intensity levels. \n",
    "\n",
    "For example, let's say we have an experiment where we see how increasing color jittering (adjusting brightness, saturationm and contrast) over time can be detected accurately, and color jitter can be on an intensity scale of [0, 1]. For task 1, we might jitter the color of images only at intensities within the windowed interval [0, 0.1]. For task 2, we might increase this window to [0, 0.2], and so on. We will be experimenting with different window sizes and whether they overlap or not, and we will save the dataset from our final reported results.\n",
    "\n",
    "### Experiments\n",
    "\n",
    "Under a general Continual Learning framework, we assume that a model will experience a series of tasks, where tasks are learned/adapted to one at a time and then tested after learning each task on all previous tasks. \n",
    "\n",
    "We will run an experiment to benchmark methods under the current general framework, which uses MVTec-AD and assumes that each category/object is its own task. This is the simplest experiment and is generally used in current literature, in some form or another. This task should be the simplest, because each category has a completely different pixel distribution space. \n",
    "\n",
    "For the different disruption types on MTD, we are running an experiment for each type of image distortion:\n",
    "- __Color Jitter__, which simulates lighting/color changes\n",
    "- __Blur__, which applies Gaussian Blur and simulates sensor wear or sensor out of focus\n",
    "- __Geometric__, which applies a mixture of rotation, translation, and shear to simulate product movement or deformation\n",
    "\n",
    "Again, the idea for these next experiments is that each task will have a similar or overlapping image pixel space, and we can test how well each method learns tasks in a continuous drift manner."
   ],
   "id": "86a85e95d262040c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from Methods.IUF.iuf import IUF_Model\n",
    "from Methods.DNE.dne import DNE_Model\n",
    "from utils.train import train_model\n",
    "from utils.eval import eval_model\n",
    "import torch\n",
    "\n",
    "# Hyperparameters for training/testing\n",
    "torch.manual_seed(42)\n",
    "# Whether to do training, with which models, and on which datasets\n",
    "TRAIN = False\n",
    "EVAL = True\n",
    "models = {\n",
    "    \"DNE\":True,\n",
    "    \"IUF\":True,\n",
    "    \"UCAD\":False\n",
    "}\n",
    "datasets = {\n",
    "    \"MVTEC\":True,\n",
    "    \"MTD\":True\n",
    "}\n",
    "NUM_EPOCHS = 25\n",
    "BATCH_SIZE = 24\n",
    "LEARNING_RATE = 0.00025\n",
    "\n",
    "data_aug = {\n",
    "    \"color\": [\n",
    "        [0.20, 0.26],\n",
    "        [0.26, 0.32],\n",
    "        [0.32, 0.38],\n",
    "        [0.38, 0.44],\n",
    "        [0.44, 0.50],\n",
    "        [0.50, 0.56],\n",
    "        [0.56, 0.62],\n",
    "        [0.62, 0.68],\n",
    "        [0.68, 0.74],\n",
    "        [0.74, 0.80]\n",
    "    ],\n",
    "    \"blur\": [\n",
    "        [1, 0.5],\n",
    "        [3, 1],\n",
    "        [5, 1.5],\n",
    "        [7, 2],\n",
    "        [9, 2.5],\n",
    "        [11, 3],\n",
    "        [13, 3.5],\n",
    "        [15, 4],\n",
    "        [17, 4.5],\n",
    "        [19, 5],\n",
    "    ],\n",
    "    \"geometric\": [\n",
    "        [4, 2, 0.02, 4],\n",
    "        [8, 4, 0.04, 8],\n",
    "        [12, 6, 0.06, 12],\n",
    "        [16, 8, 0.08, 16],\n",
    "        [20, 10, 0.10, 20],\n",
    "        [24, 12, 0.12, 24],\n",
    "        [28, 14, 0.14, 28],\n",
    "        [32, 16, 0.16, 32],\n",
    "        [36, 18, 0.18, 36],\n",
    "        [40, 20, 0.2, 40]\n",
    "    ]\n",
    "}\n",
    "# Running Training Experiments\n",
    "if TRAIN:\n",
    "    for model in models.keys():\n",
    "        if models[model]:\n",
    "            if datasets['MVTEC']:\n",
    "                train_model(model_type=model,\n",
    "                            dataset='MVTEC',\n",
    "                            num_epochs=NUM_EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            criterion=torch.nn.CrossEntropyLoss(),\n",
    "                            learning_rate=LEARNING_RATE\n",
    "                            )\n",
    "            elif datasets['MTD']:\n",
    "                for distortion in data_aug.keys():\n",
    "                    train_model(model_type=model,\n",
    "                                dataset='MTD',\n",
    "                                num_epochs=NUM_EPOCHS,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                criterion=torch.nn.CrossEntropyLoss(),\n",
    "                                learning_rate=LEARNING_RATE,\n",
    "                                tasks=data_aug[distortion],\n",
    "                                data_aug=distortion\n",
    "                                )\n",
    "\n",
    "# Running Evaluation Experiments\n",
    "if EVAL:\n",
    "    for model in models.keys():\n",
    "        if models[model]:\n",
    "            task_data = eval_model(model_type=model,\n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   data_aug=data_aug\n",
    "                                   )"
   ],
   "id": "a850d1c85d6a2bfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T15:53:27.678167Z",
     "start_time": "2025-07-07T15:53:22.054341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Methods.IUF.iuf import IUF_Model\n",
    "from utils.train import train_model\n",
    "import datasets\n",
    "from utils.eval import eval_model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "iuf = IUF_Model()\n",
    "test_dataset = datasets.mvtec(train=False, task='bottle', unsupervised=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=24, shuffle=True, collate_fn=datasets.collate)\n",
    "out = iuf.eval_one_epoch(test_dataloader)"
   ],
   "id": "92dea9adf76ac69d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T15:58:12.567287Z",
     "start_time": "2025-07-07T15:58:12.551709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for item in out[0]:\n",
    "    print(item.mean())"
   ],
   "id": "112e801acc282839",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0068)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0067)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0070)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0070)\n",
      "tensor(-0.0067)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0070)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0067)\n",
      "tensor(-0.0070)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0067)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0070)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0067)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0067)\n",
      "tensor(-0.0067)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0070)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0069)\n",
      "tensor(-0.0069)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot figure comparing experiments\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "for exp in ['supervised', 'unsupervised']:\n",
    "    dataset='MVTEC'\n",
    "    # Put together accuracy list for each experiment for plotting\n",
    "    acc_list = []\n",
    "\n",
    "    # For MVTEC\n",
    "    if dataset == \"MVTEC\":\n",
    "        for task in task_acc[dataset][exp].keys():\n",
    "            acc_list.append(task_acc[dataset][exp][task])\n",
    "        fig.add_trace(go.Scatter(y=acc_list, mode='lines',\n",
    "                                 name=f\"{exp}\"))\n",
    "    # For MTD\n",
    "    elif dataset == \"MTD\":\n",
    "        # Iterate through each data aug type\n",
    "        for aug in ['color', 'blur', 'geometric']:\n",
    "            acc_list = []\n",
    "            for task in task_acc[dataset][exp].keys():\n",
    "                if task.startswith(aug):\n",
    "                    acc_list.append(task_acc[dataset][exp][task])\n",
    "            fig.add_trace(go.Scatter(y=acc_list, mode='lines',\n",
    "                                     name=f\"MTD-{exp}-{aug}\"))\n",
    "\n",
    "\n",
    "fig.update_layout(title=\"DNE Accuracy on all Previous Tasks, MVTEC\",\n",
    "                  xaxis_title=\"Task Number\",\n",
    "                  yaxis_title=\"Accuracy on all Testing sets of previous tasks\")\n",
    "fig.show()"
   ],
   "id": "43ede93edf96922b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot figure comparing experiments\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "for dataset in [\"MVTEC\", \"MTD\"]:\n",
    "    exp='supervised'\n",
    "    # Put together accuracy list for each experiment for plotting\n",
    "    acc_list = []\n",
    "    \n",
    "    # For MVTEC\n",
    "    if dataset == \"MVTEC\":\n",
    "        for task in task_acc[dataset][exp].keys():\n",
    "            acc_list.append(task_acc[dataset][exp][task])\n",
    "        fig.add_trace(go.Scatter(y=acc_list, mode='lines', \n",
    "                                 name=f\"MVTEC-{exp}\"))\n",
    "    # For MTD\n",
    "    elif dataset == \"MTD\":\n",
    "        # Iterate through each data aug type\n",
    "        for aug in ['color', 'blur', 'geometric']:\n",
    "            acc_list = []\n",
    "            for task in task_acc[dataset][exp].keys():\n",
    "                if task.startswith(aug):\n",
    "                    acc_list.append(task_acc[dataset][exp][task])\n",
    "            fig.add_trace(go.Scatter(y=acc_list, mode='lines',\n",
    "                                     name=f\"MTD-{exp}-{aug}\"))\n",
    "                \n",
    "        \n",
    "fig.update_layout(title=\"DNE Accuracy on all Previous Tasks, Supervised Training\",\n",
    "                  xaxis_title=\"Task Number\",\n",
    "                  yaxis_title=\"Accuracy on all Testing sets of previous tasks\")\n",
    "fig.show()"
   ],
   "id": "1070ddebd5b95ca8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot figure comparing experiments\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "for dataset in [\"MVTEC\", \"MTD\"]:\n",
    "    exp='unsupervised'\n",
    "    # Put together accuracy list for each experiment for plotting\n",
    "    acc_list = []\n",
    "\n",
    "    # For MVTEC\n",
    "    if dataset == \"MVTEC\":\n",
    "        for task in task_acc[dataset][exp].keys():\n",
    "            acc_list.append(task_acc[dataset][exp][task])\n",
    "        fig.add_trace(go.Scatter(y=acc_list, mode='lines',\n",
    "                                 name=f\"MVTEC-{exp}\"))\n",
    "    # For MTD\n",
    "    elif dataset == \"MTD\":\n",
    "        # Iterate through each data aug type\n",
    "        for aug in ['color', 'blur', 'geometric']:\n",
    "            acc_list = []\n",
    "            for task in task_acc[dataset][exp].keys():\n",
    "                if task.startswith(aug):\n",
    "                    acc_list.append(task_acc[dataset][exp][task])\n",
    "            fig.add_trace(go.Scatter(y=acc_list, mode='lines',\n",
    "                                     name=f\"MTD-{exp}-{aug}\"))\n",
    "\n",
    "\n",
    "fig.update_layout(title=\"DNE Accuracy on all Previous Tasks, Unupervised Training\",\n",
    "                  xaxis_title=\"Task Number\",\n",
    "                  yaxis_title=\"Accuracy on all Testing sets of previous tasks\")\n",
    "fig.show()"
   ],
   "id": "8d9cc523b28b6670",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "task_spec = task_data.copy()\n",
    "for dataset in task_data.keys():\n",
    "    for exp in task_data[dataset].keys():\n",
    "        for task in task_data[dataset][exp].keys():\n",
    "            (preds, labels) = task_data[dataset][exp][task]\n",
    "            tn = 0\n",
    "            fp = 0\n",
    "            for i in range(len(preds)):\n",
    "                # 1 is anomaly\n",
    "                if labels[i]==1:\n",
    "                    if preds[i]==1: # tn\n",
    "                        tn += 1\n",
    "                    else:\n",
    "                        fp += 1\n",
    "                        \n",
    "            specificity = tn / (tn + fp)\n",
    "            task_spec[dataset][exp][task] = specificity\n",
    "task_spec"
   ],
   "id": "e9dea28001aa09cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot figure comparing experiments\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "for exp in ['supervised', 'unsupervised']:\n",
    "    dataset='MVTEC'\n",
    "    # Put together accuracy list for each experiment for plotting\n",
    "    acc_list = []\n",
    "\n",
    "    # For MVTEC\n",
    "    if dataset == \"MVTEC\":\n",
    "        for task in task_spec[dataset][exp].keys():\n",
    "            acc_list.append(task_spec[dataset][exp][task])\n",
    "        fig.add_trace(go.Scatter(y=acc_list, mode='lines',\n",
    "                                 name=f\"{exp}\"))\n",
    "    # For MTD\n",
    "    elif dataset == \"MTD\":\n",
    "        # Iterate through each data aug type\n",
    "        for aug in ['color', 'blur', 'geometric']:\n",
    "            acc_list = []\n",
    "            for task in task_spec[dataset][exp].keys():\n",
    "                if task.startswith(aug):\n",
    "                    acc_list.append(task_spec[dataset][exp][task])\n",
    "            fig.add_trace(go.Scatter(y=acc_list, mode='lines',\n",
    "                                     name=f\"MTD-{exp}-{aug}\"))\n",
    "\n",
    "\n",
    "fig.update_layout(title=\"DNE Sensitivity on all Previous Tasks, MVTEC\",\n",
    "                  xaxis_title=\"Task Number\",\n",
    "                  yaxis_title=\"Sensitivity on all Testing sets of previous tasks\")\n",
    "fig.show()"
   ],
   "id": "be51dc1d63cd1ced",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot figure comparing experiments\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "for dataset in [\"MVTEC\", \"MTD\"]:\n",
    "    exp='supervised'\n",
    "    # Put together accuracy list for each experiment for plotting\n",
    "    acc_list = []\n",
    "\n",
    "    # For MVTEC\n",
    "    if dataset == \"MVTEC\":\n",
    "        for task in task_spec[dataset][exp].keys():\n",
    "            acc_list.append(task_spec[dataset][exp][task])\n",
    "        fig.add_trace(go.Scatter(y=acc_list, mode='lines',\n",
    "                                 name=f\"MVTEC-{exp}\"))\n",
    "    # For MTD\n",
    "    elif dataset == \"MTD\":\n",
    "        # Iterate through each data aug type\n",
    "        for aug in ['color', 'blur', 'geometric']:\n",
    "            acc_list = []\n",
    "            for task in task_spec[dataset][exp].keys():\n",
    "                if task.startswith(aug):\n",
    "                    acc_list.append(task_spec[dataset][exp][task])\n",
    "            fig.add_trace(go.Scatter(y=acc_list, mode='lines',\n",
    "                                     name=f\"MTD-{exp}-{aug}\"))\n",
    "\n",
    "\n",
    "fig.update_layout(title=\"DNE Sensitivity on all Previous Tasks, Supervised Training\",\n",
    "                  xaxis_title=\"Task Number\",\n",
    "                  yaxis_title=\"Sensitivity on all Testing sets of previous tasks\")\n",
    "fig.show()"
   ],
   "id": "d871be780d3dbf8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot figure comparing experiments\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "for dataset in [\"MVTEC\", \"MTD\"]:\n",
    "    exp='unsupervised'\n",
    "    # Put together accuracy list for each experiment for plotting\n",
    "    acc_list = []\n",
    "\n",
    "    # For MVTEC\n",
    "    if dataset == \"MVTEC\":\n",
    "        for task in task_spec[dataset][exp].keys():\n",
    "            acc_list.append(task_spec[dataset][exp][task])\n",
    "        fig.add_trace(go.Scatter(y=acc_list, mode='lines',\n",
    "                                 name=f\"MVTEC-{exp}\"))\n",
    "    # For MTD\n",
    "    elif dataset == \"MTD\":\n",
    "        # Iterate through each data aug type\n",
    "        for aug in ['color', 'blur', 'geometric']:\n",
    "            acc_list = []\n",
    "            for task in task_spec[dataset][exp].keys():\n",
    "                if task.startswith(aug):\n",
    "                    acc_list.append(task_spec[dataset][exp][task])\n",
    "            fig.add_trace(go.Scatter(y=acc_list, mode='lines',\n",
    "                                     name=f\"MTD-{exp}-{aug}\"))\n",
    "\n",
    "\n",
    "fig.update_layout(title=\"DNE Sensitivity on all Previous Tasks, Unupervised Training\",\n",
    "                  xaxis_title=\"Task Number\",\n",
    "                  yaxis_title=\"Sensitivity on all Testing sets of previous tasks\")\n",
    "fig.show()"
   ],
   "id": "7c0b3b4c214ff9ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.count_nonzero((preds+labels)==2)",
   "id": "9a2bb026f6590b73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4e3c2a90983318ee",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
