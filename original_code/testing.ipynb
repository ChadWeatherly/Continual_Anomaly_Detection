{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Testing Original Code \n",
    "\n",
    "Found some discrepancies in code and paper description, so here we will be using a varied version of their main.py to print and keep track of exactly how data is used"
   ],
   "id": "57ebcfce11bb41e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:52:26.728613Z",
     "start_time": "2024-10-29T22:52:26.725614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from datasets import get_dataloaders\n",
    "from eval import eval_model\n",
    "from methods import get_model\n",
    "from models import get_net_optimizer_scheduler\n",
    "from utils.density import GaussianDensityTorch\n",
    "import warnings"
   ],
   "id": "c72f110f3ad3c681",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:09:25.878429Z",
     "start_time": "2024-10-29T22:09:25.461050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_inputs_labels(data):\n",
    "    \"\"\"Processes input data to handle both single-task and multi-task scenarios\n",
    "\n",
    "    This function is crucial for handling data augmentation and task transitions:\n",
    "    - For single task data (normal samples): assigns label 0\n",
    "    - For multi-task data (augmented/multiple categories): assigns sequential labels\n",
    "\n",
    "    Args:\n",
    "        data: Either a single tensor or list of tensors\n",
    "            - Single tensor: normal samples from current task\n",
    "            - List of tensors: samples from multiple tasks/augmentations\n",
    "\n",
    "    Returns:\n",
    "        tuple: (processed inputs, corresponding labels)\n",
    "    \"\"\"\n",
    "    if isinstance(data, list):\n",
    "        # Multi-task scenario: each element represents different task/augmentation\n",
    "        inputs = [x.to(args.device) for x in data]\n",
    "        # Creates sequential labels (0,1,2...) for each task\n",
    "        labels = torch.arange(len(inputs), device=args.device)\n",
    "        labels = labels.repeat_interleave(inputs[0].size(0))\n",
    "        inputs = torch.cat(inputs, dim=0)\n",
    "    else:\n",
    "        # Single task scenario: all data from same task (normal samples)\n",
    "        inputs = data.to(args.device)\n",
    "        # All normal samples get label 0\n",
    "        labels = torch.zeros(inputs.size(0), device=args.device).long()\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"\n",
    "    Alternative get_args() function used to get arguments for testing in Jupyter\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "\n",
    "    class arguments():\n",
    "        def __init__(self):\n",
    "            self.config_file = './configs/cad.yaml'\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            self.data_dir = \"../mvtec_anomaly_detection\"\n",
    "            self.mtd_dir = \"../datasets/mtd_ano_mask\"\n",
    "            self.save_checkpoint = True\n",
    "            self.save_path = \"./checkpoints\"\n",
    "            self.noise_ratio = 0\n",
    "            self.seed = 42\n",
    "\n",
    "        def str2bool(self, v):\n",
    "            return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Main training loop implementing the CAD framework\n",
    "\n",
    "    Key components:\n",
    "    1. Memory management for task statistics\n",
    "    2. Periodic evaluation during training\n",
    "    3. Support for different training methods (panda, upper bound, etc.)\n",
    "    4. Task-wise distribution tracking\n",
    "    \"\"\"\n",
    "    # Initialize model components\n",
    "    net, optimizer, scheduler = get_net_optimizer_scheduler(args)\n",
    "    density = GaussianDensityTorch()  # Used for anomaly score calculation\n",
    "    net.to(args.device)\n",
    "\n",
    "    # Get specific model implementation based on method argument\n",
    "    model = get_model(args, net, optimizer, scheduler)\n",
    "\n",
    "    # Initialize storage for tracking tasks and distributions\n",
    "    dataloaders_train = []  # Stores training dataloaders for all tasks\n",
    "    dataloaders_test = []  # Stores test dataloaders for all tasks\n",
    "    learned_tasks = []  # Keeps track of completed tasks\n",
    "    all_test_filenames = []  # Stores filenames for testing\n",
    "\n",
    "    # Statistics storage for calculating final distribution\n",
    "    task_wise_mean = []  # Stores mean embeddings per task\n",
    "    task_wise_cov = []  # Stores covariance matrices per task\n",
    "    task_wise_train_data_nums = []  # Stores number of samples per task\n",
    "\n",
    "    # Main training loop over tasks\n",
    "    for t in range(args.dataset.n_tasks):\n",
    "        print('---' * 10, f'Task:{t}', '---' * 10)\n",
    "\n",
    "        # Get dataloaders for current task and update storage\n",
    "        # First, passes empty lists\n",
    "        train_dataloader, dataloaders_train, dataloaders_test, learned_tasks, data_train_nums, all_test_filenames, train_data, test_data = \\\n",
    "            get_dataloaders(args, t, dataloaders_train, dataloaders_test, learned_tasks, all_test_filenames)\n",
    "        task_wise_train_data_nums.append(data_train_nums)\n",
    "\n",
    "        # Training loop for current task\n",
    "        # net.train()\n",
    "        # for epoch in tqdm(range(args.train.num_epochs)):\n",
    "        #     one_epoch_embeds = []  # Stores embeddings from current epoch\n",
    "        # \n",
    "        #     for batch_idx, (data) in enumerate(train_dataloader):\n",
    "        #         inputs, labels = get_inputs_labels(data)\n",
    "        #         print(labels)\n",
    "        #         break\n",
    "        #     break\n",
    "        # break\n",
    "        #     model(epoch, inputs, labels, one_epoch_embeds, t, extra_para=None)\n",
    "        #\n",
    "        # # Periodic evaluation during training\n",
    "        # if args.train.test_epochs > 0 and (epoch + 1) % args.train.test_epochs == 0:\n",
    "        #     net.eval()\n",
    "        #     # Update density estimation with current embeddings\n",
    "        #     density = model.training_epoch(\n",
    "        #         density,\n",
    "        #         one_epoch_embeds,\n",
    "        #         task_wise_mean,\n",
    "        #         task_wise_cov,\n",
    "        #         task_wise_train_data_nums,\n",
    "        #         t\n",
    "        #     )\n",
    "        #     # Evaluate model on all learned tasks\n",
    "        #     eval_model(args, epoch, dataloaders_test, learned_tasks, net, density)\n",
    "        #     net.train()\n",
    "\n",
    "    # Save final model and density estimator\n",
    "    # if args.save_checkpoint:\n",
    "    #     torch.save(net, f'{args.save_path}/net.pth')\n",
    "    #     torch.save(density, f'{args.save_path}/density.pth')\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    args = get_args()\n",
    "    main(args)"
   ],
   "id": "baab549aba691a36",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 107\u001B[0m\n\u001B[0;32m    104\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCUDA_VISIBLE_DEVICES\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m warnings\u001B[38;5;241m.\u001B[39mcatch_warnings(action\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    106\u001B[0m     \u001B[38;5;66;03m# args = get_args()\u001B[39;00m\n\u001B[1;32m--> 107\u001B[0m     main(args)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'args' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:53:11.338266Z",
     "start_time": "2024-10-29T22:53:11.332886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('configs/cad.yaml', 'r') as f:\n",
    "    data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    print(data)"
   ],
   "id": "ffc3cea863a04d04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'continual anomaly detection', 'dataset': {'name': 'seq-mvtec', 'image_size': 224, 'num_workers': 4, 'data_incre_setting': 'mul', 'n_classes_per_task': 3, 'n_tasks': 5, 'dataset_order': 1, 'strong_augmentation': False, 'random_aug': False}, 'model': {'name': 'vit', 'pretrained': True, 'method': 'dne', 'fix_head': True, 'with_dne': True, 'with_embeds': True, 'buffer_size': 200, 'n_feat': 304, 'fc_internal': 1024, 'n_coupling_blocks': 4, 'clamp': 3, 'n_scales': 3}, 'train': {'optimizer': {'name': 'adam', 'weight_decay': 3e-05, 'momentum': 0.9}, 'warmup_epochs': 10, 'warmup_lr': 0, 'base_lr': 0.0001, 'final_lr': 0, 'num_epochs': 50, 'batch_size': 32, 'test_epochs': 10, 'alpha': 0.4, 'beta': 0.5, 'num_classes': 2}, 'eval': {'eval_classifier': 'density', 'batch_size': 32, 'visualization': True}}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:53:59.205565Z",
     "start_time": "2024-10-29T22:53:59.202337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key in data:\n",
    "    print(key)"
   ],
   "id": "96d8bdd62ba2b76b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "dataset\n",
      "model\n",
      "train\n",
      "eval\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T22:54:24.890300Z",
     "start_time": "2024-10-29T22:54:24.887273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key in data['dataset']:\n",
    "    print(key)"
   ],
   "id": "3ca887347616966a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "image_size\n",
      "num_workers\n",
      "data_incre_setting\n",
      "n_classes_per_task\n",
      "n_tasks\n",
      "dataset_order\n",
      "strong_augmentation\n",
      "random_aug\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2941cc4abe8ca86a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
